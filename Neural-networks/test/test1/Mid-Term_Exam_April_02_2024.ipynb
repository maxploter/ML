{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7da187e-4d1e-4871-ba8f-4f0bcb78f5b7",
   "metadata": {},
   "source": [
    "# **LTAT.02.001 Neural Networks Mid term exam**\n",
    "<span style=\"color:red;\">Date: April 2, 2024 </span>\n",
    "\n",
    "<div style=\"background-color:rgba(0, 0, 0, 0.0470588); text-align:center; vertical-align: middle; padding:40px 0;\">\n",
    "<a href=\"/ID\">Student ID: FILL THIS IN!</a>\n",
    "</div>\n",
    "\n",
    "**To start working, create your copy 'File-> Save a copy in Drive' and work in your saved copy**\n",
    "\n",
    "**You need to submit the solved notebook through https://courses.cs.ut.ee/2024/nn/spring/Main/Tests**\n",
    "\n",
    "The exam is in two parts; programming and answering theoritical questions. \n",
    "The programming section includes basics of Neural Networks; feed forward network, backpropagation algorithm, and the theoretical concepts you have learned during the course.\n",
    "\n",
    "**You are eligible for partial points provided you support your code with enough explanation in the comment blocks**\n",
    "\n",
    "# Part I (20 points)\n",
    "\n",
    "In this problem, we will use a simple feed forward neural network to predict the probability of having brown eyes given two inputs: age, height. In other words, we have two inputs and one output predicting the **probability** of having brown eyes.\n",
    "In the following tasks, you need to do once forward-pass and backward-pass using a simple neural network which has an input layer with three input neurons, a hidden layer with three hidden neurons and an output layer with one output neuron.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb38b3bb-ccd6-4a66-887c-c5a06774936b",
   "metadata": {},
   "source": [
    "### Neural Network Initial Values\n",
    "\n",
    "\n",
    "*   Input Neurons: $x_1, x_2$\n",
    "*   Hidden Neurons: $h_1, h_2$\n",
    "*   Output Neurons: $o_1$\n",
    "*   Activation Function: custom activation function (see the formula below) in the hidden layer and no activation in the output layer.\n",
    "*   Initial weights in the first layer:\n",
    "    *  $W_{11}^{(1)} = 0.25, \\ \\  \\quad W_{12}^{(1)} = 0.15$\n",
    "    *  $W_{21}^{(1)} = 0.10,\\quad W_{22}^{(1)} = 0.1$\n",
    "    *  $W_{ij}^{(1)}$ denotes the parameter between the **ith** input neuron $x_i$ and the **jth** hidden neuron $h_j$.\n",
    "*   Initial weights in the second layer:\n",
    "    *  $W_{11}^{(2)} = 0.25$\n",
    "    *  $W_{21}^{(2)} = 0.35$\n",
    "    *  $W_{ij}^{(2)}$ denotes the parameter between the **ith** hidden neuron $h_i$ and the **jth** output neuron $o_j$.\n",
    "*   Initial learning rate is **0.1**.\n",
    "*   Target value: $t_1 = 0.6$\n",
    "*   The hidden and output neurons include a bias ($b_1^{(1)}, b_2^{(1)}$ for hidden and $b_1^{(2)}$ for output layer) of 0.\n",
    "    *  Do not forget to update their weights as well.\n",
    "*   We will use the **MSE** loss as the loss function.\n",
    "\n",
    "Hence we have only one output, **MSE loss** is\n",
    "\n",
    "$$L_{MSE}=(y - \\hat{y})^2$$\n",
    "\n",
    "where $\\hat{y}$ is the output prediction and $y$ is the target value. We are using natural logarithm for $\\log$ operations.\n",
    "\n",
    "Here are some necessary derivatives for the backpropagation algorithm.\n",
    "\n",
    "**Derivative of  MSE loss**\n",
    "\n",
    "$$\\frac{\\partial L_{MSE}}{\\partial \\hat{y}} = -2(y-\\hat{y})$$\n",
    "\n",
    "\n",
    "\n",
    "**Custom activation function in the hidden layer**:\n",
    "$$\n",
    "y = \\log(1+e^x)\n",
    "$$ &nbsp;\n",
    "\n",
    "**Derivative for activation function in the hidden layer**:\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x} = \\sigma(x)\n",
    "$$\n",
    "\n",
    "where $\\sigma(x)$ is the sigmoid function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbe2f7d",
   "metadata": {},
   "source": [
    "### Task 1.1 - Activation Function and Loss (5 points)\n",
    "\n",
    "Implement activation function, the corresponding derivative and loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5becdb01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T12:55:16.973478Z",
     "start_time": "2024-04-02T12:55:16.837426Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def activation_function(t):\n",
    "  cache = t\n",
    "  return np.log(1 + np.exp(-t)), cache\n",
    "\n",
    "def activation_function_derivative(t):\n",
    "  return 1 / (1 + np.exp(-t))\n",
    "\n",
    "def loss(P, Y):\n",
    "  return np.square(Y - P)\n",
    "\n",
    "def loss_derivative(P, Y):\n",
    "  return -2 * (Y - P)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def affine_forward(x, w, b):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for an affine (fully-connected) layer.\n",
    "\n",
    "    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N\n",
    "    examples, where each example x[i] has shape (d_1, ..., d_k). For example,\n",
    "    batch of 500 RGB CIFAR-10 images would have shape (500, 32, 32, 3). We \n",
    "    will reshape each input into a vector of dimension D = d_1 * ... * d_k,\n",
    "    and then transform it to an output vector of dimension M.\n",
    "\n",
    "    Inputs:\n",
    "    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)\n",
    "    - w: A numpy array of weights, of shape (D, M)\n",
    "    - b: A numpy array of biases, of shape (M,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: output, of shape (N, M)\n",
    "    - cache: (x, w, b)\n",
    "    \"\"\"\n",
    "    out = None\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the affine forward pass. Store the result in out. You   #\n",
    "    # will need to reshape the input into rows.                               #\n",
    "    ###########################################################################\n",
    "    out = x.reshape(x.shape[0], np.prod(x.shape[1:])) @ w + b\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    cache = (x, w, b)\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def affine_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for an affine layer.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivative, of shape (N, M)\n",
    "    - cache: Tuple of:\n",
    "      - x: Input data, of shape (N, d_1, ... d_k)\n",
    "      - w: Weights, of shape (D, M)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)\n",
    "    - dw: Gradient with respect to w, of shape (D, M)\n",
    "    - db: Gradient with respect to b, of shape (M,)\n",
    "    \"\"\"\n",
    "    x, w, b = cache\n",
    "    dx, dw, db = None, None, None\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the affine backward pass. Do not forget to reshape your #\n",
    "    # dx to match the dimensions of x.                                        #\n",
    "    ###########################################################################\n",
    "    dx = np.dot(dout, w.T).reshape(x.shape)\n",
    "    dw = np.dot(x.reshape(x.shape[0], np.prod(x.shape[1:])).T, dout)\n",
    "    db = np.sum(dout, axis=0)\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    assert dx.shape == x.shape, \"dx.shape != x.shape: \" + str(dx.shape) + \" != \" + str(x.shape)\n",
    "    assert dw.shape == w.shape, \"dw.shape != w.shape: \" + str(dw.shape) + \" != \" + str(w.shape)\n",
    "    assert db.shape == b.shape, \"db.shape != b.shape: \" + str(db.shape) + \" != \" + str(b.shape)\n",
    "\n",
    "    return dx, dw, db\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T12:55:16.978674Z",
     "start_time": "2024-04-02T12:55:16.972378Z"
    }
   },
   "id": "770513004f34a8bf",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def affine_custom_activation_forward(x, w, b, verbose=False):\n",
    "\n",
    "    a, fc_cache = affine_forward(x, w, b)\n",
    "    if verbose:\n",
    "        print(f\"hidden input: {a}\")\n",
    "    out, arctan_cache = activation_function(a)\n",
    "    if verbose:\n",
    "        print(f\"hidden output: {out}\")\n",
    "    cache = (fc_cache, arctan_cache)\n",
    "    return out, cache\n",
    "\n",
    "def affine_custom_activation_backward(dout, cache):\n",
    "    fc_cache, activation_fun_cache = cache\n",
    "    da = dout * activation_function_derivative(activation_fun_cache)\n",
    "    dx, dw, db = affine_backward(da, fc_cache)\n",
    "    return dx, dw, db"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T12:55:16.986852Z",
     "start_time": "2024-04-02T12:55:16.978816Z"
    }
   },
   "id": "54f19794e303b00a",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class TwoLayerNet(object):\n",
    "\n",
    "    def __init__(self, W1, b1,  W2, b2, verbose=False):\n",
    "\n",
    "        self.params = {}\n",
    "        \n",
    "        self.params['W1'] = W1\n",
    "        self.params['b1'] = b1\n",
    "        self.params['W2'] = W2\n",
    "        self.params['b2'] = b2\n",
    "        self.num_layers = 2\n",
    "\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def loss(self, X, y=None):\n",
    "\n",
    "        scores = None\n",
    "        hidden, cache1 = affine_custom_activation_forward(X, self.params['W1'], self.params['b1'], self.verbose)\n",
    "        scores, cache2 = affine_forward(hidden, self.params['W2'], self.params['b2'])\n",
    "\n",
    "        # If y is None then we are in test mode so just return scores\n",
    "        if y is None:\n",
    "            return scores\n",
    "\n",
    "        loss_val, grads = 0, {}\n",
    "        \n",
    "        probs = scores\n",
    "\n",
    "        # Add regularization to the loss\n",
    "        loss_val = loss(probs, y)\n",
    "\n",
    "        dscores = loss_derivative(probs, y) / y.shape[0]\n",
    "\n",
    "        # Backprop into the second layer\n",
    "        dhidden, dW2, db2 = affine_backward(dscores, cache2)\n",
    "        grads[f'W{self.num_layers}'] = dW2\n",
    "        grads[f'b{self.num_layers}'] = db2\n",
    "\n",
    "        # Backprop into the first layer\n",
    "        dx, dW1, db1 = affine_custom_activation_backward(dhidden, cache1)\n",
    "        grads['W1'] = dW1\n",
    "        grads['b1'] = db1\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f'dx: {dx}')\n",
    "\n",
    "        return loss_val, grads"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T12:55:17.010176Z",
     "start_time": "2024-04-02T12:55:16.989116Z"
    }
   },
   "id": "d1d6bacdf7b4b40b",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "34637847-a009-433a-a34a-027c0e4c3713",
   "metadata": {},
   "source": [
    "### Task 1.2 - Forward Pass (5 points)\n",
    "\n",
    "Perform a forward pass using inputs $x_1=0.3,x_2=0.6$ and given initial weight values, then update the following tables.\n",
    "\n",
    "* Input: The net input going in the corresponding neuron.\n",
    "* Output: The net output of the corresponding neuron. **Hint:** Activation function.\n",
    "* Loss (L): The loss calculated after a forward-pass.\n",
    "\n",
    "For all responses in table keep at least 3 digit precision.\n",
    "\n",
    "| Unit  | Input of unit | Output of unit |\n",
    "|-------|---------------|----------------|\n",
    "| $h_1$ |  0.135         | 0.628          |\n",
    "| $h_2$ |    0.105      | 0.642     |\n",
    "| $o_1$ | 0.382 | 0.382          |\n",
    "\n",
    "\n",
    "| Loss (L) |\n",
    "|----------|\n",
    "| 0.048    |\n",
    "\n",
    "\n",
    "You can solve the forward and backward pass tasks whichever way you like, you can:\n",
    " * Write everything in code with matrix multiplications etc, we have provided a code block for you, but feel free to write your own code.\n",
    " * Use code to do calculations one-by-one for each variable. This is basically solving it by hand, but using python as calculator. If you choose this option please make sure that you use reasonable and understandable variable names and calculation order and don't forget to comment.\n",
    " * Use actual pen and paper to write down the calculations. In this case we need you to include images of the calculations. If your handwriting is messy or not understandable, you will lose points. If possible, prefer one of the other options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bea97a7-51f5-4daf-b0a0-0881c0a46c60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T12:55:17.019399Z",
     "start_time": "2024-04-02T12:55:16.993679Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden input: [[0.135 0.105]]\n",
      "hidden output: [[0.62792358 0.64202467]]\n",
      "scores: [[0.38168953]]\n",
      "hidden input: [[0.135 0.105]]\n",
      "hidden output: [[0.62792358 0.64202467]]\n",
      "dx: [[-0.02662647 -0.01386725]]\n",
      "loss: [[0.04765946]]\n"
     ]
    }
   ],
   "source": [
    "###########\n",
    "# You can use this code block for your work or feel free to write your own code.\n",
    "\n",
    "# You may start by defining matrices\n",
    "W1 = np.array([[0.25, 0.15],[0.1, 0.10]])\n",
    "W2 = np.array([[0.25], [0.35]])\n",
    "b1 = np.array([0,0])\n",
    "b2 = np.array([0])\n",
    "learning_rate = 0.1\n",
    "\n",
    "# dataset\n",
    "X = np.array([[0.3, 0.6]])\n",
    "y = np.array([[0.6]])\n",
    "\n",
    "#####################\n",
    "## FORWARD AND LOSS##\n",
    "#####################\n",
    "\n",
    "model = TwoLayerNet(W1, b1, W2, b2, verbose=True)\n",
    "\n",
    "score = model.loss(X)\n",
    "print(f\"scores: {score}\")\n",
    "\n",
    "l, grads = model.loss(X, y)\n",
    "print(f\"loss: {l}\")\n",
    "\n",
    "#####################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f689cdcb-69a1-4aa1-aaa5-194d584f6c6e",
   "metadata": {},
   "source": [
    "<font color='red'>Report your results in the table above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb584554-1f8c-42a3-8f70-3a9ea7a6d179",
   "metadata": {},
   "source": [
    "### Task 1.3 - Backward Pass (10 points)\n",
    "\n",
    "Perform a backward pass using the loss calculated above and the parameters provided in the beginning, then, update the following tables.\n",
    "\n",
    "#### Second Layer (3 p)\n",
    "\n",
    "Your task here is to calculate delta terms or errors for the given parameters. Note that weights belong to the second layer.\n",
    "\n",
    "| Parameter  | Initial value | Derivative of loss wrt the parameter | Updated value |\n",
    "|------------|---------------|--------------------------------------|---------------|\n",
    "| $W_{11}^{(2)}$ | 0.25 | -0.274                               | 0.277         |\n",
    "| $W_{21}^{(2)}$ | 0.35 | -0.280                               | 0.378         |\n",
    "| $b_1^{(2)}$    | 0 | -0.437                               | 0.044         |\n",
    "\n",
    "#### First Layer (4 p)\n",
    "\n",
    "Your task here is to calculate delta terms or errors for the given parameters. Note that weights belong to the first layer.\n",
    "\n",
    "| Parameter  | Initial value | Derivative of loss wrt the parameter | Updated value |\n",
    "|------------|---------------|--------------------------------------|---------------|\n",
    "| $W_{12}^{(1)}$ | 0.15 | -0.024                               | 0.152         |\n",
    "| $W_{22}^{(1)}$ | 0.1 | -0.048                               | 0.105         |\n",
    "| $b_2^{(1)}$    | 0 |      -0.080                                |      0.008         |\n",
    "\n",
    "\n",
    "#### Input Layer (3 p)\n",
    "\n",
    "Your task here is to calculate delta terms or errors with respect to input neurons\n",
    "\n",
    "**We do not perform update on input**\n",
    "\n",
    "| Parameter  | Initial value | Derivative of loss wrt the parameter | Updated value |\n",
    "|------------|---------------|--------------------------------------|--------------|\n",
    "| $x_{1}$ | 0.3 | -0.027                               | n/a          |\n",
    "| $x_{2}$ | 0.6 | -0.014                               | n/a          |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4570dca0-468a-4e77-8eae-bbaa01814b17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T12:55:17.021126Z",
     "start_time": "2024-04-02T12:55:17.006393Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden input: [[0.135 0.105]]\n",
      "hidden output: [[0.62792358 0.64202467]]\n",
      "dx: [[-0.02662647 -0.01386725]]\n",
      "loss: [[0.04765946]]\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "## BACKWARD Pass ##\n",
    "###################\n",
    "\n",
    "l, grads = model.loss(X, y)\n",
    "print(f\"loss: {l}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b382c966",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T12:55:17.022277Z",
     "start_time": "2024-04-02T12:55:17.015944Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1_12: 0.15\n",
      "W1_12: 0.1\n",
      "dW1_12: -0.024124931380783286\n",
      "dW1_22: -0.04824986276156657\n",
      "W1_12_upd: 0.15241249313807834\n",
      "W1_22_upd: 0.10482498627615666\n",
      "db1_2: -0.08041643793594429\n",
      "b1_2_upd: 0.008041643793594429\n",
      "W2_12: 0.25\n",
      "W2_12: 0.35\n",
      "dW2_12: -0.2741645827984359\n",
      "dW2_22: -0.2803214162587189\n",
      "W2_12_upd: 0.27741645827984357\n",
      "W2_22_upd: 0.37803214162587184\n",
      "db2_2: -0.43662094008203256\n",
      "b2_2_upd: 0.043662094008203256\n"
     ]
    }
   ],
   "source": [
    "###########\n",
    "## Update values\n",
    "###########\n",
    "\n",
    "print(f\"W1_12: {model.params['W1'][0][1]}\")\n",
    "print(f\"W1_12: {model.params['W1'][1][1]}\")\n",
    "\n",
    "print(f\"dW1_12: {grads['W1'][0][1]}\")\n",
    "print(f\"dW1_22: {grads['W1'][1][1]}\")\n",
    "\n",
    "W1_upd = model.params['W1']\n",
    "W1_upd -= learning_rate * grads['W1']\n",
    "\n",
    "print(f\"W1_12_upd: {W1_upd[0][1]}\")\n",
    "print(f\"W1_22_upd: {W1_upd[1][1]}\")\n",
    "\n",
    "print(f\"db1_2: {grads['b1'][1]}\")\n",
    "\n",
    "b1_upd = model.params['b1']\n",
    "\n",
    "b1_upd = b1_upd - learning_rate * grads['b1']\n",
    "\n",
    "print(f\"b1_2_upd: {b1_upd[1]}\")\n",
    "\n",
    "# W2\n",
    "\n",
    "print(f\"W2_12: {model.params['W2'][0][0]}\")\n",
    "print(f\"W2_12: {model.params['W2'][1][0]}\")\n",
    "\n",
    "print(f\"dW2_12: {grads['W2'][0][0]}\")\n",
    "print(f\"dW2_22: {grads['W2'][1][0]}\")\n",
    "\n",
    "W2_upd = model.params['W2']\n",
    "W2_upd -= learning_rate * grads['W2']\n",
    "\n",
    "print(f\"W2_12_upd: {W2_upd[0][0]}\")\n",
    "print(f\"W2_22_upd: {W2_upd[1][0]}\")\n",
    "\n",
    "print(f\"db2_2: {grads['b2'][0]}\")\n",
    "\n",
    "b2_upd = model.params['b2']\n",
    "\n",
    "b2_upd = b2_upd - learning_rate * grads['b2']\n",
    "\n",
    "print(f\"b2_2_upd: {b2_upd[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c54d65",
   "metadata": {},
   "source": [
    "<font color='red'>**!!! Remember to fill answer tables and explain how you get there by either attaching picture of your solution (or giving us your paper solution) OR commenting your code and explaining your prints!!!**</font> If your answer is slightly wrong, but you reported correct logic, you still can get points!\n",
    "\n",
    "Also make sure to check indexes of values that we ask to report in table, as we are not asking to report all values from the first layer!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9825b31",
   "metadata": {},
   "source": [
    "# Part II (10 points)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Please answer the following questions in a few sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5790a78a",
   "metadata": {},
   "source": [
    "1. Does it make sense to update the input layer in the above application? why? (2 pts)\n",
    "- **Ans:** \n",
    "- Input layer is fixed, unless we're allowed to change our dataset dimentions (e.g. crop image)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0168bd2",
   "metadata": {},
   "source": [
    "2. Is our choice of output layer activation and loss functions good considering the mentioned application? please discuss **why** and mention if something different is **better suited**? (2 pts)\n",
    "- **Ans:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cb9ebf",
   "metadata": {},
   "source": [
    "3.  Together, forward & backward pass, they form one training iteration known as epoch. Assume that we have a dataset with 400 rows/ samples and we randomly selected a batch size of 5 and 2000 epochs.\n",
    "* 3a. How many batches there will be? (0.5 pts)\n",
    "- **Ans:**\n",
    "- 400/5 = 80\n",
    "* 3b. How many batches will be considered to complete an epoch? (0.5 pts)\n",
    "- **Ans:**\n",
    "- All batches should pass through consider epoch complete. So 80 batches.\n",
    "* 3c. After 2000 epochs, how many batches the model will pass through during the training process? (0.5 pts)\n",
    "- **Ans:**\n",
    "- 2_000*80 = 160_000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6be9048",
   "metadata": {},
   "source": [
    "<font color='red'>**Explain all your calculations in details, if your answer is slightly wrongm but you reported correct logic, you still can get points**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0d8239-7206-45ae-beb2-2915e3b92f06",
   "metadata": {},
   "source": [
    "4. Imagine that you have a neural network with one hidden layer with 200 nodes (a \"two-layer network\", because it has two weight matrices), and you are using arctan activation function at the nodes. The inputs to the network are RGB images of size 64x64x3. The output is 5 probability values (obtained by passing the results through Softmax activation function). How many scalar operations (additions, subtractions, multiplications, divisions, exponentiations, arctan, logarithms and comparisons) does it take to perform a forward pass on one data point (to calculate the cross-entropy loss) (2.5 pts) _Hint! Beware of biases_\n",
    "- **Ans:**\n",
    "\n",
    "\n",
    "n * m multiplications\n",
    "(n-1) * m summations \n",
    "complexity is O(max(n, m))\n",
    "\n",
    "### Input parameters\n",
    "\n",
    "D = 64*64*3\n",
    "N = 200\n",
    "C = 5\n",
    "\n",
    "### First layer\n",
    "XW1+b1\n",
    "\n",
    "D*N + (D-1)*N + N\n",
    "\n",
    "### Activation\n",
    "N - each element we apply activation function arctan\n",
    "\n",
    "### Second layer\n",
    "HW2+b2\n",
    "\n",
    "N*C + (N-1)*C + C\n",
    "\n",
    "### Softmax\n",
    "C (exponents) + C-1 (summations) + C (divisions)\n",
    "\n",
    "### Total\n",
    "\n",
    "sum all operations on each layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e21623",
   "metadata": {},
   "source": [
    "5. What is the difference between stochastic gradient descent and batch gradient descent? (1 pts)\n",
    "- **Ans:**\n",
    "- Batch Gradient Descent can be used for smoother curves. SGD can be used when the dataset is large. Batch Gradient Descent converges directly to minima. SGD converges faster for larger datasets. (https://towardsdatascience.com/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1800736d",
   "metadata": {},
   "source": [
    "6. Mention 1 application where calculating the gradient of the input is useful? (1 pts)\n",
    "- **Ans:**\n",
    "-"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
