{"cells":[{"cell_type":"markdown","metadata":{"id":"NXjPd_AVIu6T"},"source":["# Homework 7: Text classification with Pytorch\n","\n","#### Important: Save a copy of this notebook before you start working and upload the solved notebook to the HW submission page given below, as always no need to upload the datasets.\n"," - All theoretical questions must be answered in your own words, do not copy-paste text from the internet. Points can be deducted for terrible formatting or incomprehensible English.\n"," - Code must be commented. If you use code you found online, you have to add the link to the source you used. There is no penalty for using outside sources as long as you convince us you understand the code.\n","\n","**Note that this HW has only one notebook.**\n","\n","*Once completed download the ipython notebook and upload it to https://courses.cs.ut.ee/2024/nn/spring/Main/Practices.*"]},{"cell_type":"markdown","metadata":{"id":"z_NFuPA9OEhy"},"source":["## Introduction\n","\n","In this practice session we are looking into text classification. This means we are going to touch topics like word embeddings and recurrent neural networks."]},{"cell_type":"markdown","metadata":{"id":"uDH4YJZvOGHD"},"source":["Lets download the data first! This is a subset from movie reviews datasets with 25000 reviews. And corresponding positive or negative labels."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V35fj4_MqFU4"},"outputs":[],"source":["!wget -c https://courses.cs.ut.ee/2023/nn/spring/Main/HomePage?action=download\\&upname=labels.txt -O labels.txt\n","!wget -c https://courses.cs.ut.ee/2023/nn/spring/Main/HomePage?action=download\\&upname=reviews.txt -O reviews.txt\n","\n","import numpy as np\n","# read data from text files\n","\n","with open('reviews.txt', 'r') as f:\n","     reviews = f.readlines()\n","     \n","with open('labels.txt', 'r') as f:\n","     labels = f.readlines()"]},{"cell_type":"markdown","metadata":{"id":"t1JEyrg9DUkB"},"source":["# Preprocessing\n","\n","In some cases, just throwing the text as it is in an embedder and feeding it to a model might not be the best solution. So, some pre-processing steps are required to use text data properly.\n"]},{"cell_type":"markdown","metadata":{"id":"m2hiO2qhWtGH"},"source":["**Task 1** \n","\n","Clean up the reviews of all punctuation, since the focus of this is words and not special characters.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PKOND2clqdFM"},"outputs":[],"source":["# Your code here"]},{"cell_type":"markdown","metadata":{"id":"2TCHbxtckXEa"},"source":["**Task 2** \n","\n","Get word counts of the words in the reveiws and sort the words by frequency to get the most common words. Let's use a vocabulary of top (2k words)! Create a dictionary to map the position in the top 2k scoreboard to the word."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3d1mVbM_syig"},"outputs":[],"source":["# your code here \n","print(vocab_to_int) # print the dictionary"]},{"cell_type":"markdown","metadata":{"id":"Qxw65MTOrUst"},"source":["**Task 3** \n","\n","\n","However, the top 2k words are not all the possible words that there can be in a review; so now we need to take care of the words not found in the dictionary. Let's add a special character 'UNK' to denote these words. Shift the sequence and add the word 'UNK' in the dictionary in position 1 to denote the words not found this dictionary.\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1681122638702,"user":{"displayName":"Tarun Khajuria","userId":"05104667879518305264"},"user_tz":-180},"id":"fp7x3kIAthJP","outputId":"119f5d1c-60ac-4847-c584-c100a62c284c"},"outputs":[{"data":{"text/plain":["2000"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["len(vocab_to_int)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ABXpmIdgrR4M"},"outputs":[],"source":["## Your code here"]},{"cell_type":"markdown","metadata":{"id":"ua9zf5NRk9XV"},"source":["**Task 4** \n","\n","\n","Encode all the sentences in the dataset using the disctionary. Fix the sequence length to 250 words, padd with (0) or truncate sequences with less/more words. *Note that you will also have to handle the words missing in the dictionary.*  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sgbyv4wbs4J6"},"outputs":[],"source":["## Your code here"]},{"cell_type":"markdown","metadata":{"id":"-r-7C1f--Lr5"},"source":["Here we convert the labels from our data set into 1 or 0 for positive/negative"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FU5W9sTvtdM2"},"outputs":[],"source":["labels=[1 if label.strip()=='positive' else 0 for label in labels]\n"]},{"cell_type":"markdown","metadata":{"id":"X8yLc21Wzq9C"},"source":["**Task 5** \n","\n","Split the dataset into *train:val:test* sets with *80:10:10* proportions.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1681122644245,"user":{"displayName":"Tarun Khajuria","userId":"05104667879518305264"},"user_tz":-180},"id":"wyvAb3YNtfWz","outputId":"c2d5c585-05d9-4e98-db2f-3b43124ef9ce"},"outputs":[{"name":"stdout","output_type":"stream","text":["20000 2500 2500\n"]}],"source":["#split_dataset into 80% training , 10% test and 10% Validation Dataset\n","train_x= #\n","test_x=#\n","valid_x=#\n","## do the same for y\n","print(len(train_y), len(valid_y), len(test_y))"]},{"cell_type":"markdown","metadata":{"id":"GIR0RBzjz3Ay"},"source":["# Data Loading\n","\n","Now we are generating Dataset and data loaders for our train and test set. See how the dataloader will automatically create data batches for your training/validation/test routines\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DyUh6Ksytk5a"},"outputs":[],"source":["import torch\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","#create Tensor Dataset\n","train_data=TensorDataset(torch.FloatTensor(train_x), torch.FloatTensor(train_y))\n","valid_data=TensorDataset(torch.FloatTensor(valid_x), torch.FloatTensor(valid_y))\n","test_data=TensorDataset(torch.FloatTensor(test_x), torch.FloatTensor(test_y))\n","\n","#dataloader\n","batch_size=50\n","train_loader=DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","valid_loader=DataLoader(valid_data, batch_size=batch_size, shuffle=True)\n","test_loader=DataLoader(test_data, batch_size=batch_size, shuffle=True)"]},{"cell_type":"markdown","metadata":{"id":"6NdH7Eho0bPs"},"source":["# Model\n","\n","Next we create the Model class where we define our model in its init function. The forward function is what will be called when you call an instance of the model class, its the definition of forward function that defines the conputational chain of your model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nVw7ab9Ttph2"},"outputs":[],"source":["\n","import torch.nn as nn\n"," \n","class SentimentalLSTM(nn.Module):\n","    \"\"\"\n","    The RNN model that will be used to perform Sentiment analysis.\n","    \"\"\"\n","    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):    \n","        \"\"\"\n","        Initialize the model by setting up the layers\n","        \"\"\"\n","        super().__init__()\n","        self.output_size=output_size\n","        self.n_layers=n_layers\n","        self.hidden_dim=hidden_dim\n","        \n","        #Embedding and LSTM layers\n","        self.embedding=nn.Embedding(vocab_size, embedding_dim)\n","        self.lstm=nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n","        \n","        #dropout layer\n","        self.dropout=nn.Dropout(0)\n","        \n","        #Linear and sigmoid layer\n","        self.fc1=nn.Linear(hidden_dim, 64)\n","        self.fc2=nn.Linear(64, 16)\n","        self.fc3=nn.Linear(16,output_size)\n","        self.sigmoid=nn.Sigmoid()\n","        \n","    def forward(self, x):\n","        \"\"\"\n","        Perform a forward pass of our model on some input and hidden state.\n","        \"\"\"\n","        batch_size=x.size()\n","        \n","        #Embadding and LSTM output\n","        embedd=self.embedding(x)\n","        lstm_out, hidden=self.lstm(embedd)\n","        \n","        #stack up the lstm output\n","        lstm_out=lstm_out.contiguous().view(-1, self.hidden_dim)\n","        \n","        #dropout and fully connected layers\n","        out=self.dropout(lstm_out)\n","        out=self.fc1(out)\n","        out=self.dropout(out)\n","        out=self.fc2(out)\n","        out=self.dropout(out)\n","        out=self.fc3(out)\n","        sig_out=self.sigmoid(out)\n","        \n","        sig_out=sig_out.view(batch_size, -1)\n","        sig_out=sig_out[:, -1]\n","        \n","        return sig_out\n","    \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FQLDltWhDNJ7"},"outputs":[],"source":["## function to get the validation loss at some stage of the training\n","def validation(net,valid_loader,criterion):\n","    # Get validation loss\n","    val_losses = []\n","    net.eval()\n","    for inputs, labels in valid_loader:\n","        inputs, labels = inputs.cuda(), labels.cuda()  \n","        output = net(inputs.to(torch.int))\n","        val_loss = criterion(output.squeeze(), labels.float())\n","\n","        val_losses.append(val_loss.item())\n","    net.train()\n","    return val_losses\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oVzWfOWDCil5"},"outputs":[],"source":["## the main train function to call when you want to train your model\n","def train(net,train_loader,validation_loader,criterion,optimizer):\n","        # check if CUDA is available\n","    train_on_gpu = torch.cuda.is_available()\n","\n","    # training params\n","\n","    epochs = 4 # 3-4 is approx where I noticed the validation loss stop decreasing\n","\n","    counter = 0\n","    print_every = 100\n","    clip=5 # gradient clipping\n","\n","    # move model to GPU, if available\n","    if(train_on_gpu):\n","        net.cuda()\n","\n","    net.train()\n","    # train for some number of epochs\n","    for e in range(epochs):\n","\n","        # batch loop\n","        for inputs, labels in train_loader:\n","            counter += 1\n","\n","            if(train_on_gpu):\n","                inputs=inputs.cuda()\n","                labels=labels.cuda()\n","            \n","            # zero accumulated gradients\n","            net.zero_grad()\n","\n","            # get the output from the model\n","            output = net(inputs.to(torch.long))\n","\n","            # calculate the loss and perform backprop\n","            loss = criterion(output.squeeze(), labels.float())\n","            loss.backward()\n","            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n","            nn.utils.clip_grad_norm_(net.parameters(), clip)\n","            optimizer.step()\n","\n","            # loss stats\n","            if counter % print_every == 0:\n","                val_losses= validation(net,valid_loader,criterion)\n","                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n","                    \"Step: {}...\".format(counter),\n","                    \"Loss: {:.6f}...\".format(loss.item()),\n","                    \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j3rICJ3Lw7BC"},"outputs":[],"source":["## function to test your model on the test set after it is trained\n","def test(net,test_loader,criterion):\n","    test_losses = [] # track loss\n","    num_correct = 0\n","    train_on_gpu = torch.cuda.is_available()\n","\n","    net.eval()\n","    # iterate over test data\n","    for inputs, labels in test_loader:\n","\n","        if(train_on_gpu):\n","            inputs, labels = inputs.cuda(), labels.cuda()\n","\n","        output = net(inputs.to(torch.long))\n","\n","        # calculate loss\n","        test_loss = criterion(output.squeeze(), labels.float())\n","        test_losses.append(test_loss.item())\n","\n","        # convert output probabilities to predicted class (0 or 1)\n","        pred = torch.round(output.squeeze())  # rounds to the nearest integer\n","\n","        # compare predictions to true label\n","        correct_tensor = pred.eq(labels.float().view_as(pred))\n","        correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n","        num_correct += np.sum(correct)\n","\n","\n","    # -- stats! -- ##\n","    # avg test loss\n","    print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n","\n","    # accuracy over all test data\n","    test_acc = num_correct/len(test_loader.dataset)\n","    print(\"Test accuracy: {:.3f}\".format(test_acc))\n"]},{"cell_type":"markdown","metadata":{"id":"bG5cpBu2_IP6"},"source":["Here we define some model parameters and create an instance of our model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jxjeI74ft6SH"},"outputs":[],"source":["vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding\n","output_size = 1\n","embedding_dim = 256\n","hidden_dim = 80\n","n_layers = 1\n","\n","net = SentimentalLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers,drop_prob=0)\n","print(net)"]},{"cell_type":"markdown","metadata":{"id":"ZiZ5tdAAx4AC"},"source":["**Task 6** \n","\n","Explain the dimensions of the embedding layer."]},{"cell_type":"markdown","metadata":{"id":"QtIeM5p7t8nz"},"source":["*Your answer here:*"]},{"cell_type":"markdown","metadata":{"id":"pu0L_Brm_0Cf"},"source":["**Task 7** \n","\n","Update the learning rate or the model parameters in the model definition above to train the Network; the goal is to reach above 78% test accuracy. The train and test blocks are given below, run the block to check how it works."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PJGGmGT4Dy_Q"},"outputs":[],"source":["lr=0.005\n","criterion = nn.BCELoss()\n","optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n","train(net,train_loader,valid_loader,criterion,optimizer)\n","test(net,test_loader,criterion)"]},{"cell_type":"markdown","metadata":{"id":"DHsrYXE5ySb5"},"source":["**Task 8** \n","\n","Now let's switch things up! Use the model and the model definition block below and do the same computation, but instead of using LSTM in your model use simple RNN or GRU units. Do you see a difference in your models in terms of number of parameters, definition, performance, etc? Explain your answer."]},{"cell_type":"markdown","metadata":{"id":"kM5w6z8a5SEu"},"source":["*Your answer here:*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"THyKaKbTx1CH"},"outputs":[],"source":["## Edit this code block to define the model with RNN or GRU instead of LSTM\n","import torch.nn as nn\n"," \n","class SentimentalLSTM(nn.Module):\n","    \"\"\"\n","    The RNN model that will be used to perform Sentiment analysis.\n","    \"\"\"\n","    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):    \n","        \"\"\"\n","        Initialize the model by setting up the layers\n","        \"\"\"\n","        super().__init__()\n","        self.output_size=output_size\n","        self.n_layers=n_layers\n","        self.hidden_dim=hidden_dim\n","        \n","        #Embedding and LSTM layers\n","        self.embedding=nn.Embedding(vocab_size, embedding_dim)\n","        self.lstm=nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n","        \n","        #dropout layer\n","        self.dropout=nn.Dropout(0)\n","        \n","        #Linear and sigmoid layer\n","        self.fc1=nn.Linear(hidden_dim, 64)\n","        self.fc2=nn.Linear(64, 16)\n","        self.fc3=nn.Linear(16,output_size)\n","        self.sigmoid=nn.Sigmoid()\n","        \n","    def forward(self, x):\n","        \"\"\"\n","        Perform a forward pass of our model on some input and hidden state.\n","        \"\"\"\n","        batch_size=x.size()\n","        \n","        #Embadding and LSTM output\n","        embedd=self.embedding(x)\n","        lstm_out, hidden=self.lstm(embedd)\n","        \n","        #stack up the lstm output\n","        lstm_out=lstm_out.contiguous().view(-1, self.hidden_dim)\n","        \n","        #dropout and fully connected layers\n","        out=self.dropout(lstm_out)\n","        out=self.fc1(out)\n","        out=self.dropout(out)\n","        out=self.fc2(out)\n","        out=self.dropout(out)\n","        out=self.fc3(out)\n","        sig_out=self.sigmoid(out)\n","        \n","        sig_out=sig_out.view(batch_size, -1)\n","        sig_out=sig_out[:, -1]\n","        \n","        return sig_out\n"]},{"cell_type":"markdown","metadata":{"id":"FqtBFYcp5fXD"},"source":["Create instance of the new model and run train/test your model. You may need to change the learning rate and/or no of epochs to get your model to train well."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U3sGZKxo01Lo"},"outputs":[],"source":["vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding\n","output_size = 1\n","embedding_dim = 256\n","hidden_dim = 80\n","n_layers = 1\n","\n","net = SentimentalLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers,drop_prob=0)\n","print(net)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-mQ-9vf63iY5"},"outputs":[],"source":["lr=0.001\n","criterion = nn.BCELoss()\n","optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n","train(net,train_loader,valid_loader,criterion,optimizer)\n","test(net,test_loader,criterion)"]},{"cell_type":"markdown","metadata":{"id":"bg1hDmN_4gVn"},"source":["**Task 9** \n","\n","Generate positive and negative senteces. Try to get scores above 0.8 for positive sentences, and below 0.2 for negative sentences."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gL2yrbdJ4fh3"},"outputs":[],"source":["inputs = [] ## write your reviews here, in quotes, seperated by commas \n","\n","encoded = np.zeros((len(inputs),250),int)\n","for sent_idx,sent in enumerate(inputs):\n","    for idx,word in enumerate(sent.split()):\n","        if(word not in vocab_to_int.keys()):\n","            encoded[sent_idx,idx]=1\n","        else:\n","            encoded[sent_idx,idx]=vocab_to_int[word]\n","encoded = torch.from_numpy(encoded)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":573,"status":"ok","timestamp":1681109473663,"user":{"displayName":"Tarun Khajuria","userId":"05104667879518305264"},"user_tz":-180},"id":"pMJbgx2647dy","outputId":"656ebfa8-71eb-464c-8c30-633b65a8e38a"},"outputs":[{"name":"stdout","output_type":"stream","text":["True\n"]}],"source":["net.eval()\n","if(train_on_gpu):\n","    encoded= encoded.cuda()\n","output= net(encoded.to(torch.long))"]},{"cell_type":"markdown","metadata":{"id":"niBeYPDNBw3J"},"source":["**Task 10** \n","\n","Convert the output into positive or negative with 0.5 threshold (i.e. >0.5 is positive)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jCdbu2pVAwH7"},"outputs":[],"source":["## Your code here"]},{"cell_type":"markdown","metadata":{"id":"UOw7b_eTAxmc"},"source":["**Task 11** \n","\n","How performant would your rate your AI? What would you change if you were tasked to improve this model? Expain. *Note that it could be anything in terms of dataset, data pre-processing, Neural Networks architecture etc.*"]},{"cell_type":"markdown","metadata":{"id":"3ORuB9xRBUk6"},"source":["*Your answer here:*"]},{"cell_type":"markdown","metadata":{"id":"XShK1wgk64j5"},"source":["Now lets try to visualise how your network has learnt to embed the words in the vocabulary. We will first exract the weights of the embedding layer. Use PCA to project them into a 2D space. And then visualise the projection of some words in the 2d space. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fQEq2-2IE4Up"},"outputs":[],"source":["embed= net.embedding.weight.cpu().detach().numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KYiuDu4qE_m1"},"outputs":[],"source":["from sklearn.decomposition import PCA\n","embed_2d = PCA(n_components = 2).fit_transform(embed)\n","embed_2d.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Oow_zDOxFoUh"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","plt.figure(figsize=(16,8))\n","word_idx=[]\n","for word in [\"great\",\"awesome\",\"beautiful\",\"magnificent\",\"masterpiece\"]:\n","    idx = vocab_to_int[word]\n","    word_idx.append(idx)\n","    plt.scatter(embed_2d[idx,0],embed_2d[idx,1],color=\"red\",s=100)\n","    plt.text(embed_2d[idx, 0], embed_2d[idx, 1], word,fontsize=20)\n","for word in [\"bad\", \"terrible\", \"boring\", \"lame\"]:\n","    idx = vocab_to_int[word]\n","    word_idx.append(idx)\n","    plt.scatter(embed_2d[idx,0],embed_2d[idx,1],color=\"blue\",s=100)\n","    plt.text(embed_2d[idx, 0], embed_2d[idx, 1], word,fontsize=20)\n","\n","for word in [\"actor\", \"producer\", \"director\", \"dog\",\"and\",\"the\"]:\n","    idx = vocab_to_int[word]\n","    word_idx.append(idx)\n","    plt.scatter(embed_2d[idx,0],embed_2d[idx,1],color=\"gray\",s=100)\n","    plt.text(embed_2d[idx, 0], embed_2d[idx, 1], word,fontsize=20)\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"lkLeIGmMGny1"},"source":["**Task 12**\n","\n","Explain what the distribution of words using the learnt work emeddings shows?"]},{"cell_type":"markdown","metadata":{"id":"G5jBQLpQ60bb"},"source":["*Your answer here:*"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
